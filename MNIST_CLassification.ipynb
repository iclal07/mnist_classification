{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN1zj+fZiWmz/NnDrSBi3ey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iclal07/mnist_classification/blob/main/MNIST_CLassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Necesseary Libraries"
      ],
      "metadata": {
        "id": "SEJUa5WMk5qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "DHubrUw6NtvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load and Preprocess the Data"
      ],
      "metadata": {
        "id": "DODItmrClBcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "WrACp7ECzsaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape and normalize the images\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "Dgoy3jgbNtsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample_images(images, num_images=10):\n",
        "    random_indices = np.random.choice(images.shape[0], num_images, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(images[idx].reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "display_sample_images(train_images)"
      ],
      "metadata": {
        "id": "jw_5WfLwzLw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample_images(images, num_images=10):\n",
        "    # Rastgele seçilen resimleri gösterme\n",
        "    random_indices = np.random.choice(images.shape[0], num_images, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(images[idx].reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "display_sample_images(test_images)"
      ],
      "metadata": {
        "id": "Yg764gR_zOjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[:10])\n",
        "print(test_labels[:10])"
      ],
      "metadata": {
        "id": "qnOBsUcEzSJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the  First Model"
      ],
      "metadata": {
        "id": "yfTlNVB2lFjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "bcMUknHUNtpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the Model"
      ],
      "metadata": {
        "id": "tlWBJWf8lHeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ZvI3C7zgNtmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "t8jYROGElSbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)"
      ],
      "metadata": {
        "id": "NaxoXYeHN4ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "Ho8erL3MlU-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "id": "i77BQrVyN5zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_images)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "\n",
        "# Actual tags\n",
        "true_labels = tf.argmax(test_labels, axis=1) if test_labels.shape[1] > 1 else test_labels.squeeze()\n",
        "\n",
        "# Calculate the Precision, Recall and F1 Score\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "\n",
        "print(f'First Model Evaluation Results')\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "a4FS-R228sHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    # Kayıp değerlerini çizdirme\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss Evolution')\n",
        "\n",
        "    # Doğruluk değerlerini çizdirme\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy Evolution')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Daha önce eğitilen modelin history nesnesini fonksiyona gönderme\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "Fz0pYVK0mXmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "SxPuDfDQll6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "datagen.fit(train_images)"
      ],
      "metadata": {
        "id": "QoJlFKChSn8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_augmented = model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
        "                    epochs=5, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "IGULzxnNS584"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss,test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "\n",
        "# Actual tags\n",
        "true_labels = tf.argmax(test_labels, axis=1) if test_labels.shape[1] > 1 else test_labels.squeeze()\n",
        "\n",
        "# Calculate the Precision, Recall and F1 Score\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "\n",
        "print(f'Augmented Model Evaluation Results')\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "BUacushrl4l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history_augmented)"
      ],
      "metadata": {
        "id": "GPWtyzs4mewg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When we compare the graphs and results before and after the data augmentation, we observe that the validation data reaches more accurate results with less loss.**\n",
        "\n",
        "```\n",
        "First Model Evaluation Results\n",
        "Test accuracy: 0.992900013923645\n",
        "Test Loss: 0.025905493646860123\n",
        "Precision: 0.9914354600269231\n",
        "Recall: 0.9916046542045006\n",
        "F1 Score: 0.991496038151271\n",
        "```\n",
        "\n",
        "```\n",
        "Augmented Model Evaluation Results\n",
        "Test accuracy: 0.9930999875068665\n",
        "Test Loss: 0.024184685200452805\n",
        "Precision: 0.9931073779251923\n",
        "Recall: 0.993067469539857\n",
        "F1 Score: 0.9930631384516981\n",
        "```"
      ],
      "metadata": {
        "id": "-hQmVvF-5QdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deeper CNN:**"
      ],
      "metadata": {
        "id": "bJs48Gr7Tbzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_deeper = models.Sequential()\n",
        "model_deeper.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model_deeper.add(layers.MaxPooling2D((2, 2)))\n",
        "model_deeper.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_deeper.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_deeper.add(layers.MaxPooling2D((2, 2)))\n",
        "model_deeper.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_deeper.add(layers.Flatten())\n",
        "model_deeper.add(layers.Dense(128, activation='relu'))\n",
        "model_deeper.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "u2kMTPTLS9OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and Train the Deeper CNN Model"
      ],
      "metadata": {
        "id": "pb07FU3Cl-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_deeper.compile(optimizer='adam',\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "history_deeper = model_deeper.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                                  epochs=5, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "fZwwTlyBXgb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss,test_acc = model_deeper.evaluate(test_images, test_labels)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "\n",
        "# Actual tags\n",
        "true_labels = tf.argmax(test_labels, axis=1) if test_labels.shape[1] > 1 else test_labels.squeeze()\n",
        "\n",
        "# Calculate the Precision, Recall and F1 Score\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "\n",
        "print(f'Deeper CNN Model Evaluation Results')\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "ci5B7NnFmHgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history_deeper)"
      ],
      "metadata": {
        "id": "QwRyzQuymiXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Augmented Model Evaluation Results\n",
        "Test accuracy: 0.9930999875068665\n",
        "Test Loss: 0.024184685200452805\n",
        "Precision: 0.9931073779251923\n",
        "Recall: 0.993067469539857\n",
        "F1 Score: 0.9930631384516981\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Deeper CNN Model Evaluation Results\n",
        "Test accuracy: 0.9937999844551086\n",
        "Test Loss: 0.018411941826343536\n",
        "Precision: 0.9931073779251923\n",
        "Recall: 0.993067469539857\n",
        "F1 Score: 0.9930631384516981\n",
        "```\n",
        "\n",
        "Although there is not much difference in accuracy between depeer cnn and the first model, we can say that our deeper cnn model is better because the loss in the depper cnn model is less.\n",
        "\n"
      ],
      "metadata": {
        "id": "hOvvjwLbG1J0"
      }
    }
  ]
}